# Default configuration for miniwdl runner; see:
#   https://miniwdl.readthedocs.io/en/latest/runner_reference.html#configuration
# For guidance on overriding these defaults with a custom .cfg file and/or environment variables.
#
# This file is organized into sections & options within each section. Some values take the form of
# a JSON object or array, which may span multiple -indented- lines. Except within JSON, string
# values don't need to be in quotes.


[scheduler]
# Size of thread pools for task and subworkflow call management. Bounds how many WDL tasks the
# runner may attempt to schedule concurrently; actual concurrency may be less due to CPU/memory
# resource availability. This setting may need to be increased on multi-node deployments, but not
# beyond ~200 (guideline) to avoid excessive overhead in miniwdl's Python process.
# 0 = default to host `nproc`.
# -@
call_concurrency = 0
# container backend (pluggable)
container_backend = docker_swarm
# When one task fails, immediately terminate all other running tasks. If disabled, stop launching
# new tasks, but leave those still running to succeed or fail on their own. The latter mode might
# be useful with call caching (see below) to avoid discarding all work done by other tasks.
fail_fast = true


[docker_swarm]
# Docker Swarm Mode is the default container backend. It's included with every docker installation,
# but disabled by default. The following option allows miniwdl to automatically initialize the local
# host as a one-node swarm if needed, by performing
#     docker swarm init --listen-addr 127.0.0.1 --advertise-addr 127.0.0.1
# This is is typically helpful for single-node use, but should be disabled if a preexisting swarm
# is to be used.
auto_init = true
# Period for polling the status of running containers. Increasing this reduces steady-state load on
# miniwdl and dockerd (when many tasks are running concurrently), but also increases the delay
# noticing when tasks have exited and (--verbose) the appearance of standard error logs. Worker
# threads randomize the interval +/- 50% to spread out activity.
polling_period_seconds = 1.5
# Retry idempotent dockerd requests yielding 5xx status code (after polling_period_seconds)
server_error_retries = 2


[file_io]
# During startup, require the run directory and input files to reside somewhere within this root
# directory. This constraint is needed in cluster deployments relying on a shared mount. It can
# also be useful on a single node to prevent accidental consumption of a small boot/home volume,
# by confining workflow operations to some spacious scratch/data mount.
root = /
# Populate task working directory with writable copies of the input files, instead of mounting them
# in situ & read-only. Needed if tasks want to write/move/rename input files, but costs time and
# disk space. --copy-input-files
copy_input_files = false
# Each succeeded run directory has an "out/" folder containing (by default) a symbolic link to each
# output file in its original working location. If output_hardlinks is true, then out/ is populated
# with hardlinks instead of symlinks. Beware the potential confusion arising from files with
# multiple hardlinks! See also delete_work, below.
output_hardlinks = false
# Delete task working directory upon completion. The task container's working directory is a
# bind-mounted host directory, so files written into it are left behind after the container is torn
# down. If tasks write large non-output files into their working directory (instead of $TMPDIR as
# they should), then it can be useful to delete them automatically.
# Values:
#   false   = never delete (default)
#   success = delete working directories of succeeded tasks
#   failure =            "                  failed tasks
#   always  =            "                  both succeeded and failed tasks
# The "success" and "always" settings require output_hardlinks, above, to be true; otherwise,
# output files would be deleted too. Input/output JSON, logs, and stdout/stderr are always retained
# in the task run directory (above the container working directory).
delete_work = false


[task_runtime]
# Effective maximum values of runtime.cpu and runtime.memory (bytes), which evaluated values may be
# rounded down to in order to "fit" the maximum available host resources. Warning: tasks may
# deadlock if these are set higher than actual achievable resources.
# 0 = detect host resources, -1 = do not apply a limit.
# --runtime-cpu-max, --runtime-memory-max
cpu_max = 0
memory_max = 0
# A task's runtime.memory is used as a "reservation" to guide container scheduling, but isn't an
# enforced limit unless memory_limit_multiplier is positive, which sets a hard limit of
# memory_limit_multiplier*runtime.memory. Recommendation: if activating this, disable host swap.
memory_limit_multiplier = 0.0
# Defaults which each task's runtime{} section will be merged into. --runtime-defaults
defaults = {
        "docker": "ubuntu:20.04"
    }
# Run the command script as the invoking user's uid:gid instead of usually running as root. More
# secure, but interferes with commands that assume root access e.g. apt-get. --as-me
as_user = false


[download_cache]
# When File or Directory inputs are given URIs to be downloaded, store the downloaded copy in a
# local directory where it can later be found and reused for the same input URI.
put = false
# Enable retrieval of File/Directory input URIs from the local cache
get = false
# Base directory for the local download cache.
dir = /tmp/miniwdl_download_cache
# Remove URI query strings for cache key/lookup purposes. By default, downloads from URIs with
# query strings are never cached (neither put nor get).
ignore_query = false
# To be eligible for caching (in addition to above options), a URI must (i) match at least one glob
# pattern in enable_patterns, AND (ii) not match any disable_patterns.
enable_patterns = ["*"]
disable_patterns = ["*.php", "*.aspx"]


[download_aria2c]
# aria2c is used to download https:// and ftp:// input URIs
docker = hobbsau/aria2@sha256:9d4646b1576e88e4061f509b25b8c0c6e1a5fface4ac37d9ecab67de5f7d9a4b


[download_awscli]
# If workflow inputs or generates s3:// URIs, load AWS credentials using boto3 on the miniwdl host.
# Note: If running inside EC2, downloader & other tasks might be able assume an IAM role via the
#       instance metadata service, regardless of this setting. You can set the environment variable
#       AWS_EC2_METADATA_DISABLED=true to prevent this (or use iptables to block the endpoint IP).
# Recommendation: use only trusted WDL and Docker images, if either host_credentials = true or an
#                 EC2 instance profile is available.
# Failing all of the above, public S3 URIs can always be used.
host_credentials = false
docker = amazon/aws-cli@sha256:66bee4012645113dae8aaa587d07b5945bfa03c2c3d6aeadd383a46b2198a276


[download_gsutil]
# docker pull gcr.io/google.com/cloudsdktool/cloud-sdk:slim
# docker inspect gcr.io/google.com/cloudsdktool/cloud-sdk:slim | jq .
docker = gcr.io/google.com/cloudsdktool/cloud-sdk@sha256:ceaa1a5062de27b340d52f895e869109096251b58ccc35ad74ea3cafc043c6a4


[call_cache]
# When a task in a workflow is performed, cache the output of the task in a certain directory where it can
# be found later and reused for the same task definition/inputs
put = false
# enable retrieval of cached outputs
get = false
# pluggable implementation: the default stores cache JSON files in a local directory, and checks
# posix mtimes of any local files referenced in the cached inputs/outputs (invalidating the cache
# entry if any referenced files are newer)
backend = dir
dir = ~/.cache/miniwdl


[plugins]
# Control which plugins are used. Plugins are installed using the Python entry points convention,
#   https://packaging.python.org/specifications/entry-points/
# Furthermore for a plugin to be used, its "object reference" must (i) match at least one glob
# pattern in enable_patterns, AND (ii) not match any disable_patterns.
enable_patterns = ["*"]
disable_patterns = ["miniwdl_task_omnibus_example:*"]
